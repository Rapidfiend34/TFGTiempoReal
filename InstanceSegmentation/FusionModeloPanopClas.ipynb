{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando segmentación mejorada...\n",
      "Cargando modelo DeepLab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\anaconda3\\envs\\ia2023\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aaron\\anaconda3\\envs\\ia2023\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando imagen...\n",
      "Error al procesar la imagen: No se pudo cargar la imagen: C:\\Users\\aaron\\Documents\\Año4\\TFG\\CityScapesWithPanopticSegm\\dataset\\images\\test\\berlin_000009_000019_gtFine_polygons.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "\n",
    "def process_image(image_path, model, device='cpu'):\n",
    "    try:\n",
    "        # Cargar y preparar imagen\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"No se pudo cargar la imagen: {image_path}\")\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detectar carretera y acera\n",
    "        road_mask, sidewalk_mask = detect_road_and_sidewalk(image_rgb)\n",
    "        \n",
    "        # Preparar para DeepLab\n",
    "        transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        input_tensor = transform(image_rgb).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)['out']\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "        \n",
    "        semantic_mask = torch.argmax(probs[0], dim=0).cpu().numpy()\n",
    "        \n",
    "        # Detectar persona\n",
    "        person_mask = semantic_mask == 15\n",
    "        \n",
    "        # Crear máscara semántica final\n",
    "        semantic_mask_final = np.zeros_like(semantic_mask)\n",
    "        semantic_mask_final[road_mask] = 7        # carretera\n",
    "        semantic_mask_final[sidewalk_mask] = 11   # acera\n",
    "        semantic_mask_final[person_mask] = 15     # persona\n",
    "        \n",
    "        # Crear máscara de instancias solo para personas\n",
    "        instance_mask = np.zeros_like(semantic_mask)\n",
    "        person_binary = person_mask.astype(np.uint8)\n",
    "        num_labels, labels = cv2.connectedComponents(person_binary)\n",
    "        \n",
    "        instance_info = {}\n",
    "        for i in range(1, num_labels):\n",
    "            y, x = np.where(labels == i)\n",
    "            if len(x) > 100:  # Filtrar regiones pequeñas\n",
    "                instance_mask[labels == i] = i\n",
    "                instance_info[i] = {\n",
    "                    'class_id': 15,\n",
    "                    'class_name': 'person',\n",
    "                    'bbox': [x.min(), y.min(), x.max(), y.max()],\n",
    "                    'confidence': 0.95,\n",
    "                    'area': len(x)\n",
    "                }\n",
    "        \n",
    "        return image_rgb, instance_mask, semantic_mask_final, instance_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar la imagen: {str(e)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def create_panoptic_visualization(image, instance_mask, semantic_mask, instance_info):\n",
    "    # Colores para la visualización\n",
    "    class_colors = {\n",
    "        0: [0, 0, 0],         # background\n",
    "        7: [128, 64, 128],    # road - gris azulado\n",
    "        11: [244, 35, 232],   # sidewalk - rosa\n",
    "        15: [220, 20, 60]     # person - rojo\n",
    "    }\n",
    "    \n",
    "    # Crear visualización base con máscaras semánticas\n",
    "    vis = np.zeros_like(image)\n",
    "    \n",
    "    # Aplicar primero las máscaras semánticas de carretera y acera\n",
    "    vis[semantic_mask == 7] = class_colors[7]   # carretera\n",
    "    vis[semantic_mask == 11] = class_colors[11] # acera\n",
    "    \n",
    "    # Aplicar las instancias de personas\n",
    "    for instance_id in instance_info:\n",
    "        mask = instance_mask == instance_id\n",
    "        vis[mask] = class_colors[15]  # persona siempre en rojo\n",
    "    \n",
    "    # Mezclar con la imagen original\n",
    "    alpha = 0.6\n",
    "    final_vis = cv2.addWeighted(image, 1-alpha, vis.astype(np.uint8), alpha, 0)\n",
    "    \n",
    "    # Dibujar bounding boxes solo para personas\n",
    "    for instance_id, info in instance_info.items():\n",
    "        x1, y1, x2, y2 = info['bbox']\n",
    "        color = tuple(map(int, class_colors[15]))  # color rojo para personas\n",
    "        cv2.rectangle(final_vis, (x1, y1), (x2, y2), color, 2)\n",
    "        \n",
    "        # Añadir etiqueta\n",
    "        label = f\"Person ({info['confidence']:.2f})\"\n",
    "        cv2.putText(final_vis, label, (x1, y1-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    return final_vis\n",
    "\n",
    "def detect_road_and_sidewalk(image):\n",
    "    \"\"\"Detecta carretera y acera usando técnicas de procesamiento de imagen\"\"\"\n",
    "    # Convertir a escala de grises\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Altura y ancho de la imagen\n",
    "    h, w = gray.shape\n",
    "    \n",
    "    # Crear máscaras\n",
    "    road_mask = np.zeros_like(gray)\n",
    "    sidewalk_mask = np.zeros_like(gray)\n",
    "    \n",
    "    # Región de interés para la carretera (parte inferior central)\n",
    "    road_mask[int(h*0.6):h, int(w*0.2):int(w*0.8)] = 1\n",
    "    \n",
    "    # Región de interés para la acera (bandas laterales)\n",
    "    sidewalk_mask[int(h*0.4):h, :int(w*0.3)] = 1\n",
    "    sidewalk_mask[int(h*0.4):h, int(w*0.7):] = 1\n",
    "    \n",
    "    return road_mask > 0, sidewalk_mask > 0\n",
    "\n",
    "def visualize_segmentation(image_path):\n",
    "    print(\"Cargando modelo DeepLab...\")\n",
    "    model = deeplabv3_resnet101(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Procesando imagen...\")\n",
    "    image, instance_mask, semantic_mask, instance_info = process_image(image_path, model)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    panoptic_vis = create_panoptic_visualization(image, instance_mask, semantic_mask, instance_info)\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Imagen Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(semantic_mask, cmap='nipy_spectral')\n",
    "    plt.title('Máscara Semántica')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(panoptic_vis)\n",
    "    plt.title('Segmentación Panóptica')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimir información solo de las instancias de personas\n",
    "    print(\"\\nPersonas detectadas:\")\n",
    "    for instance_id, info in instance_info.items():\n",
    "        print(f\"Persona {instance_id}:\")\n",
    "        print(f\"  Confianza: {info['confidence']:.2f}\")\n",
    "        print(f\"  Área: {info['area']} píxeles\")\n",
    "        print(f\"  Bounding Box: {info['bbox']}\")\n",
    "        print()\n",
    "    \n",
    "    # Imprimir información de las máscaras semánticas\n",
    "    unique_classes = np.unique(semantic_mask)\n",
    "    print(\"Máscaras semánticas:\")\n",
    "    class_names = {0: \"Fondo\", 7: \"Carretera\", 11: \"Acera\", 15: \"Persona\"}\n",
    "    for class_id in unique_classes:\n",
    "        pixels = np.sum(semantic_mask == class_id)\n",
    "        percentage = (pixels / semantic_mask.size) * 100\n",
    "        print(f\"{class_names.get(class_id, f'Clase {class_id}')}: {percentage:.2f}% de la imagen\")\n",
    "\n",
    "print(\"Iniciando segmentación mejorada...\")\n",
    "visualize_segmentation(r'C:\\Users\\aaron\\Documents\\Año4\\TFG\\CityScapesWithPanopticSegm\\dataset\\images\\test\\berlin_000009_000019_gtFine_polygons.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia2023",
   "language": "python",
   "name": "ia2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
